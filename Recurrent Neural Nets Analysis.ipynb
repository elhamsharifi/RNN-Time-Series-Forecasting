{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Time Series Forecasting with Recurrent Neural Nets\n",
    "\n",
    "# Part III: Time series analysis with recurrent neural nets\n",
    "\n",
    "The remaining parts of the project will build models for time series data using tensorflow.\n",
    "\n",
    "We will be using the mortgage affordability provided by Zillow https://www.zillow.com/research/data/\n",
    "\n",
    "The project repository includes the data and utility functions to load and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>SizeRank</th>\n",
       "      <th>1979-03</th>\n",
       "      <th>1979-06</th>\n",
       "      <th>1979-09</th>\n",
       "      <th>1979-12</th>\n",
       "      <th>1980-03</th>\n",
       "      <th>1980-06</th>\n",
       "      <th>1980-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2014-12</th>\n",
       "      <th>2015-03</th>\n",
       "      <th>2015-06</th>\n",
       "      <th>2015-09</th>\n",
       "      <th>2015-12</th>\n",
       "      <th>2016-03</th>\n",
       "      <th>2016-06</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>394913</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.278316</td>\n",
       "      <td>0.284399</td>\n",
       "      <td>0.319438</td>\n",
       "      <td>0.379845</td>\n",
       "      <td>0.318970</td>\n",
       "      <td>0.337586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241961</td>\n",
       "      <td>0.240494</td>\n",
       "      <td>0.247387</td>\n",
       "      <td>0.245957</td>\n",
       "      <td>0.249606</td>\n",
       "      <td>0.241544</td>\n",
       "      <td>0.239503</td>\n",
       "      <td>0.240021</td>\n",
       "      <td>0.265376</td>\n",
       "      <td>0.269264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>753899</td>\n",
       "      <td>Los Angeles-Long Beach-Anaheim, CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.357694</td>\n",
       "      <td>0.388161</td>\n",
       "      <td>0.401762</td>\n",
       "      <td>0.459422</td>\n",
       "      <td>0.541508</td>\n",
       "      <td>0.464528</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383162</td>\n",
       "      <td>0.382531</td>\n",
       "      <td>0.396277</td>\n",
       "      <td>0.395686</td>\n",
       "      <td>0.402116</td>\n",
       "      <td>0.393042</td>\n",
       "      <td>0.390708</td>\n",
       "      <td>0.386895</td>\n",
       "      <td>0.430334</td>\n",
       "      <td>0.433476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>394463</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.261928</td>\n",
       "      <td>0.276790</td>\n",
       "      <td>0.276248</td>\n",
       "      <td>0.309709</td>\n",
       "      <td>0.353099</td>\n",
       "      <td>0.299344</td>\n",
       "      <td>0.295616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134913</td>\n",
       "      <td>0.134063</td>\n",
       "      <td>0.139074</td>\n",
       "      <td>0.137962</td>\n",
       "      <td>0.138784</td>\n",
       "      <td>0.134412</td>\n",
       "      <td>0.133701</td>\n",
       "      <td>0.132856</td>\n",
       "      <td>0.147025</td>\n",
       "      <td>0.149054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>394514</td>\n",
       "      <td>Dallas-Fort Worth, TX</td>\n",
       "      <td>4</td>\n",
       "      <td>0.301131</td>\n",
       "      <td>0.328453</td>\n",
       "      <td>0.340857</td>\n",
       "      <td>0.386321</td>\n",
       "      <td>0.452818</td>\n",
       "      <td>0.376912</td>\n",
       "      <td>0.394715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118163</td>\n",
       "      <td>0.117663</td>\n",
       "      <td>0.124935</td>\n",
       "      <td>0.128167</td>\n",
       "      <td>0.133517</td>\n",
       "      <td>0.131539</td>\n",
       "      <td>0.133219</td>\n",
       "      <td>0.135142</td>\n",
       "      <td>0.149861</td>\n",
       "      <td>0.152387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>394974</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>5</td>\n",
       "      <td>0.204333</td>\n",
       "      <td>0.215107</td>\n",
       "      <td>0.219247</td>\n",
       "      <td>0.247690</td>\n",
       "      <td>0.295396</td>\n",
       "      <td>0.248018</td>\n",
       "      <td>0.260259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141915</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.142831</td>\n",
       "      <td>0.141625</td>\n",
       "      <td>0.142089</td>\n",
       "      <td>0.137387</td>\n",
       "      <td>0.135599</td>\n",
       "      <td>0.135064</td>\n",
       "      <td>0.148746</td>\n",
       "      <td>0.148559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RegionID                          RegionName  SizeRank   1979-03  \\\n",
       "324    394913                        New York, NY         1  0.261700   \n",
       "325    753899  Los Angeles-Long Beach-Anaheim, CA         2  0.357694   \n",
       "326    394463                         Chicago, IL         3  0.261928   \n",
       "327    394514               Dallas-Fort Worth, TX         4  0.301131   \n",
       "328    394974                    Philadelphia, PA         5  0.204333   \n",
       "\n",
       "      1979-06   1979-09   1979-12   1980-03   1980-06   1980-09    ...     \\\n",
       "324  0.278316  0.284399  0.319438  0.379845  0.318970  0.337586    ...      \n",
       "325  0.388161  0.401762  0.459422  0.541508  0.464528  0.486750    ...      \n",
       "326  0.276790  0.276248  0.309709  0.353099  0.299344  0.295616    ...      \n",
       "327  0.328453  0.340857  0.386321  0.452818  0.376912  0.394715    ...      \n",
       "328  0.215107  0.219247  0.247690  0.295396  0.248018  0.260259    ...      \n",
       "\n",
       "      2014-12   2015-03   2015-06   2015-09   2015-12   2016-03   2016-06  \\\n",
       "324  0.241961  0.240494  0.247387  0.245957  0.249606  0.241544  0.239503   \n",
       "325  0.383162  0.382531  0.396277  0.395686  0.402116  0.393042  0.390708   \n",
       "326  0.134913  0.134063  0.139074  0.137962  0.138784  0.134412  0.133701   \n",
       "327  0.118163  0.117663  0.124935  0.128167  0.133517  0.131539  0.133219   \n",
       "328  0.141915  0.139100  0.142831  0.141625  0.142089  0.137387  0.135599   \n",
       "\n",
       "      2016-09   2016-12   2017-03  \n",
       "324  0.240021  0.265376  0.269264  \n",
       "325  0.386895  0.430334  0.433476  \n",
       "326  0.132856  0.147025  0.149054  \n",
       "327  0.135142  0.149861  0.152387  \n",
       "328  0.135064  0.148746  0.148559  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import proj3_lib.utils as utils\n",
    "\n",
    "afford_df = utils.get_affordability_df()\n",
    "afford_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils.plot_affordability_series(afford_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data ready\n",
    "\n",
    "We will build models that forecast mortgage affordability based on the previous year's affordability values. First, we will split the data into a training and testing set. We take data from 2015-2017 as the test set, and all previous data as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>SizeRank</th>\n",
       "      <th>1979-03</th>\n",
       "      <th>1979-06</th>\n",
       "      <th>1979-09</th>\n",
       "      <th>1979-12</th>\n",
       "      <th>1980-03</th>\n",
       "      <th>1980-06</th>\n",
       "      <th>1980-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2012-09</th>\n",
       "      <th>2012-12</th>\n",
       "      <th>2013-03</th>\n",
       "      <th>2013-06</th>\n",
       "      <th>2013-09</th>\n",
       "      <th>2013-12</th>\n",
       "      <th>2014-03</th>\n",
       "      <th>2014-06</th>\n",
       "      <th>2014-09</th>\n",
       "      <th>2014-12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>394913</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.278316</td>\n",
       "      <td>0.284399</td>\n",
       "      <td>0.319438</td>\n",
       "      <td>0.379845</td>\n",
       "      <td>0.318970</td>\n",
       "      <td>0.337586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224406</td>\n",
       "      <td>0.217553</td>\n",
       "      <td>0.224845</td>\n",
       "      <td>0.241536</td>\n",
       "      <td>0.260265</td>\n",
       "      <td>0.261124</td>\n",
       "      <td>0.258286</td>\n",
       "      <td>0.253598</td>\n",
       "      <td>0.252561</td>\n",
       "      <td>0.241961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>753899</td>\n",
       "      <td>Los Angeles-Long Beach-Anaheim, CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.357694</td>\n",
       "      <td>0.388161</td>\n",
       "      <td>0.401762</td>\n",
       "      <td>0.459422</td>\n",
       "      <td>0.541508</td>\n",
       "      <td>0.464528</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304163</td>\n",
       "      <td>0.300080</td>\n",
       "      <td>0.320920</td>\n",
       "      <td>0.359241</td>\n",
       "      <td>0.399297</td>\n",
       "      <td>0.402892</td>\n",
       "      <td>0.400288</td>\n",
       "      <td>0.393393</td>\n",
       "      <td>0.395619</td>\n",
       "      <td>0.383162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>394463</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.261928</td>\n",
       "      <td>0.276790</td>\n",
       "      <td>0.276248</td>\n",
       "      <td>0.309709</td>\n",
       "      <td>0.353099</td>\n",
       "      <td>0.299344</td>\n",
       "      <td>0.295616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121624</td>\n",
       "      <td>0.117433</td>\n",
       "      <td>0.121061</td>\n",
       "      <td>0.131412</td>\n",
       "      <td>0.141545</td>\n",
       "      <td>0.142888</td>\n",
       "      <td>0.141657</td>\n",
       "      <td>0.138968</td>\n",
       "      <td>0.140140</td>\n",
       "      <td>0.134913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>394514</td>\n",
       "      <td>Dallas-Fort Worth, TX</td>\n",
       "      <td>4</td>\n",
       "      <td>0.301131</td>\n",
       "      <td>0.328453</td>\n",
       "      <td>0.340857</td>\n",
       "      <td>0.386321</td>\n",
       "      <td>0.452818</td>\n",
       "      <td>0.376912</td>\n",
       "      <td>0.394715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104923</td>\n",
       "      <td>0.101957</td>\n",
       "      <td>0.105754</td>\n",
       "      <td>0.114501</td>\n",
       "      <td>0.123787</td>\n",
       "      <td>0.125153</td>\n",
       "      <td>0.123385</td>\n",
       "      <td>0.121437</td>\n",
       "      <td>0.121808</td>\n",
       "      <td>0.118163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>394974</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>5</td>\n",
       "      <td>0.204333</td>\n",
       "      <td>0.215107</td>\n",
       "      <td>0.219247</td>\n",
       "      <td>0.247690</td>\n",
       "      <td>0.295396</td>\n",
       "      <td>0.248018</td>\n",
       "      <td>0.260259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138880</td>\n",
       "      <td>0.134923</td>\n",
       "      <td>0.139083</td>\n",
       "      <td>0.148469</td>\n",
       "      <td>0.156956</td>\n",
       "      <td>0.155812</td>\n",
       "      <td>0.153202</td>\n",
       "      <td>0.149514</td>\n",
       "      <td>0.148801</td>\n",
       "      <td>0.141915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     RegionID                          RegionName  SizeRank   1979-03  \\\n",
       "324    394913                        New York, NY         1  0.261700   \n",
       "325    753899  Los Angeles-Long Beach-Anaheim, CA         2  0.357694   \n",
       "326    394463                         Chicago, IL         3  0.261928   \n",
       "327    394514               Dallas-Fort Worth, TX         4  0.301131   \n",
       "328    394974                    Philadelphia, PA         5  0.204333   \n",
       "\n",
       "      1979-06   1979-09   1979-12   1980-03   1980-06   1980-09    ...     \\\n",
       "324  0.278316  0.284399  0.319438  0.379845  0.318970  0.337586    ...      \n",
       "325  0.388161  0.401762  0.459422  0.541508  0.464528  0.486750    ...      \n",
       "326  0.276790  0.276248  0.309709  0.353099  0.299344  0.295616    ...      \n",
       "327  0.328453  0.340857  0.386321  0.452818  0.376912  0.394715    ...      \n",
       "328  0.215107  0.219247  0.247690  0.295396  0.248018  0.260259    ...      \n",
       "\n",
       "      2012-09   2012-12   2013-03   2013-06   2013-09   2013-12   2014-03  \\\n",
       "324  0.224406  0.217553  0.224845  0.241536  0.260265  0.261124  0.258286   \n",
       "325  0.304163  0.300080  0.320920  0.359241  0.399297  0.402892  0.400288   \n",
       "326  0.121624  0.117433  0.121061  0.131412  0.141545  0.142888  0.141657   \n",
       "327  0.104923  0.101957  0.105754  0.114501  0.123787  0.125153  0.123385   \n",
       "328  0.138880  0.134923  0.139083  0.148469  0.156956  0.155812  0.153202   \n",
       "\n",
       "      2014-06   2014-09   2014-12  \n",
       "324  0.253598  0.252561  0.241961  \n",
       "325  0.393393  0.395619  0.383162  \n",
       "326  0.138968  0.140140  0.134913  \n",
       "327  0.121437  0.121808  0.118163  \n",
       "328  0.149514  0.148801  0.141915  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = utils.split_train_test(afford_df)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>SizeRank</th>\n",
       "      <th>2015-03</th>\n",
       "      <th>2015-06</th>\n",
       "      <th>2015-09</th>\n",
       "      <th>2015-12</th>\n",
       "      <th>2016-03</th>\n",
       "      <th>2016-06</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>394913</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1</td>\n",
       "      <td>0.240494</td>\n",
       "      <td>0.247387</td>\n",
       "      <td>0.245957</td>\n",
       "      <td>0.249606</td>\n",
       "      <td>0.241544</td>\n",
       "      <td>0.239503</td>\n",
       "      <td>0.240021</td>\n",
       "      <td>0.265376</td>\n",
       "      <td>0.269264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>753899</td>\n",
       "      <td>Los Angeles-Long Beach-Anaheim, CA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.382531</td>\n",
       "      <td>0.396277</td>\n",
       "      <td>0.395686</td>\n",
       "      <td>0.402116</td>\n",
       "      <td>0.393042</td>\n",
       "      <td>0.390708</td>\n",
       "      <td>0.386895</td>\n",
       "      <td>0.430334</td>\n",
       "      <td>0.433476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>394463</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.134063</td>\n",
       "      <td>0.139074</td>\n",
       "      <td>0.137962</td>\n",
       "      <td>0.138784</td>\n",
       "      <td>0.134412</td>\n",
       "      <td>0.133701</td>\n",
       "      <td>0.132856</td>\n",
       "      <td>0.147025</td>\n",
       "      <td>0.149054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>394514</td>\n",
       "      <td>Dallas-Fort Worth, TX</td>\n",
       "      <td>4</td>\n",
       "      <td>0.117663</td>\n",
       "      <td>0.124935</td>\n",
       "      <td>0.128167</td>\n",
       "      <td>0.133517</td>\n",
       "      <td>0.131539</td>\n",
       "      <td>0.133219</td>\n",
       "      <td>0.135142</td>\n",
       "      <td>0.149861</td>\n",
       "      <td>0.152387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>394974</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>5</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.142831</td>\n",
       "      <td>0.141625</td>\n",
       "      <td>0.142089</td>\n",
       "      <td>0.137387</td>\n",
       "      <td>0.135599</td>\n",
       "      <td>0.135064</td>\n",
       "      <td>0.148746</td>\n",
       "      <td>0.148559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RegionID                          RegionName  SizeRank   2015-03  \\\n",
       "324    394913                        New York, NY         1  0.240494   \n",
       "325    753899  Los Angeles-Long Beach-Anaheim, CA         2  0.382531   \n",
       "326    394463                         Chicago, IL         3  0.134063   \n",
       "327    394514               Dallas-Fort Worth, TX         4  0.117663   \n",
       "328    394974                    Philadelphia, PA         5  0.139100   \n",
       "\n",
       "      2015-06   2015-09   2015-12   2016-03   2016-06   2016-09   2016-12  \\\n",
       "324  0.247387  0.245957  0.249606  0.241544  0.239503  0.240021  0.265376   \n",
       "325  0.396277  0.395686  0.402116  0.393042  0.390708  0.386895  0.430334   \n",
       "326  0.139074  0.137962  0.138784  0.134412  0.133701  0.132856  0.147025   \n",
       "327  0.124935  0.128167  0.133517  0.131539  0.133219  0.135142  0.149861   \n",
       "328  0.142831  0.141625  0.142089  0.137387  0.135599  0.135064  0.148746   \n",
       "\n",
       "      2017-03  \n",
       "324  0.269264  \n",
       "325  0.433476  \n",
       "326  0.149054  \n",
       "327  0.152387  \n",
       "328  0.148559  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 144)\n",
      "(75, 9)\n"
     ]
    }
   ],
   "source": [
    "train_data= train_df.drop(['RegionName', 'SizeRank', 'RegionID'], axis=1).values\n",
    "test_data= test_df.drop(['RegionName', 'SizeRank', 'RegionID'], axis=1).values\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Recurrent Neural Nets\n",
    "# RNN\n",
    "## Use the tf.nn.dynamic_rnn and tf.nn.rnn_cell.BasicRNNCell functions to build a basic recurrent neural net to solve the affordability prediction problem. A couple of notes:\n",
    "### Setup input/target pairs as follows: for sequence x_0, x_1, x_2, x_3, \\ldots, x_m, use targets as x_1, x_2, x_3, \\ldots, xm, x{m+1}.\n",
    "### Use truncated back propagation through time (use about 8 timesteps for each backprop), remember to carry state forward between truncated backprop chunks.\n",
    "### To make predictions for county j over the test set period, you should pass the training sequence (time series) for that county through the RNN so that state is used properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I first reset the graph and then build the graph. I use 8 timesteps as truncated backprop chuncks. I use tf.contrib.rnn.OutputProjectionWrapper for cell to transform the n_neurons cells to one. I just use 50 iterations but if we use more iterations the results will improve. \n",
    "### I used these references in this part:\n",
    "####     - https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n",
    "####     -https://www.tensorflow.org/tutorials/recurrent#truncated-backpropagation\n",
    "####     - https://www.tensorflow.org/tutorials/recurrent#truncated-backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on Test Data= [ 0.00985166  0.01507053  0.00419663  0.00711916  0.00387961  0.00359939\n",
      "  0.00543834  0.00848203  0.00357559  0.00732664  0.01723113  0.00435127\n",
      "  0.00923536  0.00540945  0.00607572  0.00348713  0.0151413   0.00436873\n",
      "  0.00564725  0.00586718  0.00772886  0.00427587  0.00792591  0.00461137\n",
      "  0.00861033  0.00527136  0.00425602  0.00396999  0.00388337  0.00610631\n",
      "  0.00353847  0.0045512   0.01822483  0.00477435  0.00672561  0.00471239\n",
      "  0.00554695  0.00470306  0.0037141   0.00424363  0.00562415  0.00493963\n",
      "  0.00416846  0.00554335  0.00548873  0.01460411  0.00433963  0.00609928\n",
      "  0.00427012  0.00416327  0.00551484  0.00517231  0.00567318  0.00473823\n",
      "  0.00386649  0.00362335  0.00729644  0.00374404  0.0095823   0.00495519\n",
      "  0.00558974  0.00438712  0.00487304  0.00629621  0.01529427  0.00359596\n",
      "  0.00529133  0.008425    0.0045935   0.0089673   0.00354437  0.00908049\n",
      "  0.01344069  0.00856618  0.00349115]\n",
      "Average of MAE= 0.00642095337311\n",
      "Standard Deviation of MAE= 0.00339829440711\n"
     ]
    }
   ],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "reset_graph()\n",
    "\n",
    "total_train_length=train_data.shape[1]\n",
    "n_inputs=1\n",
    "n_neurons=100\n",
    "n_outputs=1\n",
    "n_epochs=50\n",
    "chunck_steps=8\n",
    "num_chuncks=total_train_length/chunck_steps\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, chunck_steps,n_inputs], name='input_placeholder')\n",
    "y = tf.placeholder(tf.float32, [None, chunck_steps,n_outputs], name='labels_placeholder')\n",
    "init_state = tf.zeros([1, n_neurons])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.relu),output_size=n_outputs)\n",
    "final_state=init_state\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, X, initial_state=final_state,dtype=tf.float32 )\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "learning_rate=0.001\n",
    "loss=tf.reduce_mean(tf.abs(rnn_outputs-y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        MAE_set=[]\n",
    "        i=0 \n",
    "\n",
    "        for inputs in train_data:\n",
    "          \n",
    "            for chunck in range(int(num_chuncks)): \n",
    "                X_batch=inputs[chunck*chunck_steps:(chunck+1)*chunck_steps]\n",
    "                X_batch=X_batch.reshape((-1,chunck_steps,n_inputs))\n",
    "                if chunck < int(num_chuncks)-1:\n",
    "                    y_batch=inputs[chunck*chunck_steps+1:(chunck+1)*chunck_steps+1]\n",
    "                if chunck == int(num_chuncks)-1:\n",
    "                    y_batch=np.append(inputs[chunck*chunck_steps+1:(chunck+1)*chunck_steps+1],test_data[i,0])\n",
    "\n",
    "                y_batch=y_batch.reshape((-1,chunck_steps,n_outputs))\n",
    "\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "                mae=loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                #print(i,chunck, \"\\tMAE:\", mae)\n",
    "            X_new=test_data[i,0:8]    \n",
    "            X_new=X_new.reshape((-1,chunck_steps,n_inputs))\n",
    "            y_new=test_data[i,1:9]\n",
    "            y_new=y_new.reshape((-1,chunck_steps,n_inputs))\n",
    "            #print(y_new)\n",
    "            y_pred=sess.run(rnn_outputs, feed_dict={X:X_new})\n",
    "            #print(y_pred)\n",
    "            MAE=loss.eval(feed_dict={X:X_new, y:y_new})\n",
    "            #print(\"MAE on Test Data\", i, \"=\", MAE)\n",
    "            MAE_set=np.append(MAE_set,MAE)\n",
    "            i+=1\n",
    "        if (epoch==49):    \n",
    "            print(\"MAE on Test Data=\",MAE_set)\n",
    "            print(\"Average of MAE=\", np.mean(MAE_set))\n",
    "            print(\"Standard Deviation of MAE=\", np.std(MAE_set) )   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "### Do the same as above using tf.nn.rnn_cell.BasicLSTMCellto use the LSTM model. Also look attf.nn.rnn_cell.MultiLSTMCell to build deep LSTM network\n",
    "\n",
    "## Here I build basic LSTM with just one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on Test Data= [ 0.00873547  0.01573171  0.00512684  0.00667009  0.00449899  0.00290796\n",
      "  0.00611302  0.00689097  0.00410824  0.00897991  0.01642117  0.00482043\n",
      "  0.00828072  0.00625571  0.00724943  0.00437695  0.01295693  0.00341084\n",
      "  0.00582734  0.00495953  0.00739597  0.00238627  0.0070089   0.00393627\n",
      "  0.00752453  0.00453218  0.0034133   0.00326491  0.00290527  0.00834396\n",
      "  0.00444787  0.00377234  0.01778782  0.00502433  0.0058052   0.00401571\n",
      "  0.00346731  0.00316935  0.00314541  0.00322458  0.00412143  0.00498856\n",
      "  0.00296428  0.00414536  0.00439742  0.01554278  0.00343555  0.00574521\n",
      "  0.00489257  0.00332741  0.0044419   0.00555747  0.00563371  0.00359005\n",
      "  0.00277062  0.0032036   0.00739498  0.00320676  0.00727938  0.00368422\n",
      "  0.00486239  0.00439631  0.00579891  0.00811361  0.01579919  0.00321093\n",
      "  0.00509102  0.00736043  0.00384183  0.0081352   0.00324007  0.00726874\n",
      "  0.01164765  0.00707213  0.00312851]\n",
      "Average of MAE= 0.00597578617434\n",
      "Standard Deviation of MAE= 0.00343247788012\n"
     ]
    }
   ],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "reset_graph()\n",
    "\n",
    "total_train_length=train_data.shape[1]\n",
    "n_inputs=1\n",
    "n_neurons=100\n",
    "n_outputs=1\n",
    "n_epochs=50\n",
    "chunck_steps=8\n",
    "num_chuncks=total_train_length/chunck_steps\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, chunck_steps,n_inputs], name='input_placeholder')\n",
    "y = tf.placeholder(tf.float32, [None, chunck_steps,n_outputs], name='labels_placeholder')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "lstm_cell = tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicLSTMCell(n_neurons),output_size=n_outputs)\n",
    "\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=lstm_cell,\n",
    "                                   inputs=X,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "learning_rate=0.001\n",
    "loss=tf.reduce_mean(tf.abs(outputs-y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        MAE_set=[]\n",
    "        i=0 \n",
    "\n",
    "        for inputs in train_data:\n",
    "\n",
    "            for chunck in range(int(num_chuncks)): \n",
    "                X_batch=inputs[chunck*chunck_steps:(chunck+1)*chunck_steps]\n",
    "                X_batch=X_batch.reshape((-1,chunck_steps,n_inputs))\n",
    "                if chunck < int(num_chuncks)-1:\n",
    "                    y_batch=inputs[chunck*chunck_steps+1:(chunck+1)*chunck_steps+1]\n",
    "                if chunck == int(num_chuncks)-1:\n",
    "                    y_batch=np.append(inputs[chunck*chunck_steps+1:(chunck+1)*chunck_steps+1],test_data[i,0])\n",
    "\n",
    "                y_batch=y_batch.reshape((-1,chunck_steps,n_outputs))\n",
    "\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "                mae=loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                #print(i,chunck, \"\\tMAE:\", mae)\n",
    "            X_new=test_data[i,0:8]    \n",
    "            X_new=X_new.reshape((-1,chunck_steps,n_inputs))\n",
    "            y_new=test_data[i,1:9]\n",
    "            y_new=y_new.reshape((-1,chunck_steps,n_inputs))\n",
    "            #print(y_new)\n",
    "            y_pred=sess.run(outputs, feed_dict={X:X_new})\n",
    "            #print(y_pred)\n",
    "            MAE=loss.eval(feed_dict={X:X_new, y:y_new})\n",
    "            #print(\"MAE on Test Data\", i, \"=\", MAE)\n",
    "            MAE_set=np.append(MAE_set,MAE)\n",
    "            i+=1\n",
    "        if (epoch==49):    \n",
    "            print(\"MAE on Test Data=\",MAE_set)\n",
    "            print(\"Average of MAE=\", np.mean(MAE_set))\n",
    "            print(\"Standard Deviation of MAE=\", np.std(MAE_set) )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I build multi LSTM with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on Test Data= [ 0.01016437  0.01653736  0.00398009  0.00528836  0.00459133  0.0028228\n",
      "  0.00738354  0.00844439  0.00334629  0.00852444  0.01465988  0.00791115\n",
      "  0.00801934  0.0056774   0.00714596  0.00460641  0.013224    0.00610596\n",
      "  0.00478674  0.00469626  0.00723619  0.00391236  0.004901    0.00408586\n",
      "  0.00792013  0.00491752  0.00355871  0.00374068  0.00506581  0.00738937\n",
      "  0.00395161  0.00460182  0.01722602  0.00516756  0.00607558  0.00450329\n",
      "  0.00350253  0.00460772  0.00363526  0.00405008  0.00467305  0.00353363\n",
      "  0.00386669  0.00564894  0.00523234  0.01379528  0.00434224  0.00600349\n",
      "  0.00715543  0.00371959  0.00519215  0.00413011  0.00441213  0.00542333\n",
      "  0.00359068  0.0035362   0.00757689  0.00335291  0.01078339  0.00556382\n",
      "  0.00351798  0.00408383  0.00529185  0.00767436  0.01408565  0.00557907\n",
      "  0.00551017  0.00708768  0.00351748  0.0082474   0.0028021   0.0078242\n",
      "  0.01203807  0.00763683  0.00328326]\n",
      "Average of MAE= 0.00623569848637\n",
      "Standard Deviation of MAE= 0.00320791075668\n"
     ]
    }
   ],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "reset_graph()\n",
    "\n",
    "total_train_length=train_data.shape[1]\n",
    "n_inputs=1\n",
    "n_neurons_size=[200,100]\n",
    "n_outputs=1\n",
    "n_epochs=50\n",
    "\n",
    "chunck_steps=8\n",
    "num_chuncks=total_train_length/chunck_steps\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, chunck_steps,n_inputs], name='input_placeholder')\n",
    "y = tf.placeholder(tf.float32, [None, chunck_steps,n_outputs], name='labels_placeholder')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "\n",
    "lstm_layers = [tf.contrib.rnn.OutputProjectionWrapper(tf.contrib.rnn.BasicLSTMCell(n_neurons),output_size=n_outputs) for n_neurons in n_neurons_size]\n",
    "multi_lstm_cell = tf.contrib.rnn.MultiRNNCell(lstm_layers)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=multi_lstm_cell,\n",
    "                                   inputs=X,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "learning_rate=0.001\n",
    "loss=tf.reduce_mean(tf.abs(outputs-y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        MAE_set=[]\n",
    "        i=0 \n",
    "\n",
    "        for inputs in train_data:\n",
    "\n",
    "            for chunck in range(int(num_chuncks)): \n",
    "                X_batch=inputs[chunck*chunck_steps:(chunck+1)*chunck_steps]\n",
    "                X_batch=X_batch.reshape((-1,chunck_steps,n_inputs))\n",
    "                if chunck < int(num_chuncks)-1:\n",
    "                    y_batch=inputs[chunck*chunck_steps+1:(chunck+1)*chunck_steps+1]\n",
    "                if chunck == int(num_chuncks)-1:\n",
    "                    y_batch=np.append(inputs[chunck*chunck_steps+1:(chunck+1)*chunck_steps+1],test_data[i,0])\n",
    "\n",
    "                y_batch=y_batch.reshape((-1,chunck_steps,n_outputs))\n",
    "\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "                mae=loss.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                #print(i,chunck, \"\\tMAE:\", mae)\n",
    "            X_new=test_data[i,0:8]    \n",
    "            X_new=X_new.reshape((-1,chunck_steps,n_inputs))\n",
    "            y_new=test_data[i,1:9]\n",
    "            y_new=y_new.reshape((-1,chunck_steps,n_inputs))\n",
    "            #print(y_new)\n",
    "            y_pred=sess.run(outputs, feed_dict={X:X_new})\n",
    "            #print(y_pred)\n",
    "            MAE=loss.eval(feed_dict={X:X_new, y:y_new})\n",
    "            #print(\"MAE on Test Data\", i, \"=\", MAE)\n",
    "            MAE_set=np.append(MAE_set,MAE)\n",
    "            i+=1\n",
    "        if (epoch==49):    \n",
    "            print(\"MAE on Test Data=\",MAE_set)\n",
    "            print(\"Average of MAE=\", np.mean(MAE_set))\n",
    "            print(\"Standard Deviation of MAE=\", np.std(MAE_set) )       \n",
    "            \n",
    "        \n",
    "      \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Discussion\n",
    "## Discuss the models you have used so far. Which proved to be most effective, and why? What were challenges in using these methods? What would you wish to try if you had more time to work on this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the models I fit in this project the RNN with truncated backprop and Basic LSTM and Multi LSTM that we used in this part have best results comparing with previous models that we fit. In these three models with just 50 iterations I got better results from Basic LSTM. \n",
    "\n",
    "### The baseline model and linear regression are very straight forward models but we need to play around with the neural network, RNN and also LSTM models a lot to find the best architecture. There are a lot of hyper parameters like number of layers, number of neurons in each layer, number of steps in truncated back prop and number of iterations that we need to play with and see which combination performs best.\n",
    "### If I had more time, first of all I would have tried to find good combination of hyper parameters that give good results. Also I would have learned how to use Keras and used it to solve this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V: Bonus question: how would you use these models to make longer-horizon forecasts (e.g., two or more years out)\n",
    "\n",
    "### I would use an Encoder Decoder Network to make longer-horizon forecasts. For example for two years forecast I would use 8 steps truncated back prop and then forecast for next truncated step which is 8 steps (2 years). I would use it for the whole training set to train the model and then use the model on test data to see how are the errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
