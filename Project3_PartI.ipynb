{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Introduction to TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_X, boston_y = load_boston(return_X_y=True)\n",
    "nobs, nfeatures = boston_X.shape\n",
    "boston_y = boston_y.reshape(nobs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Least Squares Regression\n",
    "### Let's use tensor flow to implement a gradient descent solution for the penalized least squares regression problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first setup data and optimization parameters and variables\n",
    "\n",
    "X = tf.constant(np.c_[np.ones(nobs), boston_X], dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(boston_y, dtype=tf.float32, name=\"y\")\n",
    "\n",
    "lam = 1.0\n",
    "penalty_vec = np.vstack([[0.0], np.full((nfeatures, 1), lam)])\n",
    "\n",
    "learning_rate = 0.000001\n",
    "nepochs = 1000\n",
    "\n",
    "stddev = 2 / np.sqrt(nfeatures)\n",
    "\n",
    "theta = tf.Variable(tf.truncated_normal([nfeatures + 1, 1], stddev=stddev), name=\"theta\")\n",
    "\n",
    "f = tf.matmul(X, theta, name=\"f\")\n",
    "r = y - f\n",
    "loss = 0.5 * tf.reduce_mean(tf.square(r), name=\"loss\")\n",
    "obj = loss + 0.5 * tf.matmul(tf.transpose(theta), penalty_vec * theta)\n",
    "\n",
    "grad = - (tf.matmul(tf.transpose(X), r / nobs)) + penalty_vec * theta\n",
    "gradnorm = tf.norm(grad)\n",
    "\n",
    "\n",
    "update = tf.assign(theta, theta - learning_rate * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 obj= [[ 48131.44140625]] grad_norm= 169148.0\n",
      "Epoch 100 obj= [[ 125.82846069]] grad_norm= 1046.76\n",
      "Epoch 200 obj= [[ 97.02219391]] grad_norm= 277.868\n",
      "Epoch 300 obj= [[ 90.41902161]] grad_norm= 245.32\n",
      "Epoch 400 obj= [[ 84.77169037]] grad_norm= 230.105\n",
      "Epoch 500 obj= [[ 79.79210663]] grad_norm= 216.248\n",
      "Epoch 600 obj= [[ 75.38844299]] grad_norm= 203.498\n",
      "Epoch 700 obj= [[ 71.4835968]] grad_norm= 191.759\n",
      "Epoch 800 obj= [[ 68.01160431]] grad_norm= 180.948\n",
      "Epoch 900 obj= [[ 64.91586304]] grad_norm= 170.986\n",
      "[[ 0.67546397]\n",
      " [-0.67017102]\n",
      " [ 0.46850663]\n",
      " [-0.08006571]\n",
      " [ 0.57550502]\n",
      " [ 0.02331622]\n",
      " [ 0.75861716]\n",
      " [ 0.190152  ]\n",
      " [ 0.06409112]\n",
      " [-0.20756727]\n",
      " [ 0.02193321]\n",
      " [-0.09849752]\n",
      " [ 0.00208634]\n",
      " [-0.33416644]]\n",
      "Training set errror 61.2541\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"obj=\", obj.eval(), \"grad_norm=\", gradnorm.eval())\n",
    "        sess.run(update)\n",
    "        \n",
    "    solution = theta.eval()\n",
    "    print(solution)\n",
    "    print(\"Training set errror\", loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, instead of providing gradients directly, use numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 obj= [[ 34935.296875]] grad_norm= 144182.0\n",
      "Epoch 100 obj= [[ 115.98401642]] grad_norm= 868.475\n",
      "Epoch 200 obj= [[ 96.95607758]] grad_norm= 210.812\n",
      "Epoch 300 obj= [[ 93.12779236]] grad_norm= 189.115\n",
      "Epoch 400 obj= [[ 89.67462158]] grad_norm= 182.618\n",
      "Epoch 500 obj= [[ 86.44981384]] grad_norm= 176.536\n",
      "Epoch 600 obj= [[ 83.43518066]] grad_norm= 170.714\n",
      "Epoch 700 obj= [[ 80.61523438]] grad_norm= 165.135\n",
      "Epoch 800 obj= [[ 77.97582245]] grad_norm= 159.786\n",
      "Epoch 900 obj= [[ 75.5039978]] grad_norm= 154.652\n",
      "[[-0.4660655 ]\n",
      " [-0.44155982]\n",
      " [-0.18564823]\n",
      " [ 0.74719751]\n",
      " [ 0.23611598]\n",
      " [-0.30666822]\n",
      " [ 0.49736616]\n",
      " [-0.27598262]\n",
      " [ 0.0386317 ]\n",
      " [-0.55132735]\n",
      " [ 0.04128872]\n",
      " [ 0.49269521]\n",
      " [ 0.05408758]\n",
      " [-0.51907086]]\n",
      "Training set error 72.1461\n"
     ]
    }
   ],
   "source": [
    "auto_grad = tf.gradients(obj, [theta])[0]\n",
    "gradnorm = tf.norm(auto_grad)\n",
    "auto_update = tf.assign(theta, theta - learning_rate * auto_grad)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"obj=\", obj.eval(), \"grad_norm=\", gradnorm.eval())\n",
    "        sess.run(auto_update)\n",
    "    solution = theta.eval()\n",
    "    print(solution)\n",
    "    print(\"Training set error\", loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, use an optimizer provided by tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.28548339]\n",
      " [-0.22648376]\n",
      " [-0.46971554]\n",
      " [-0.42798772]\n",
      " [ 0.29443103]\n",
      " [ 0.1560858 ]\n",
      " [-0.83637637]\n",
      " [-1.11538863]\n",
      " [-1.05088902]\n",
      " [ 0.21619606]\n",
      " [ 0.94561756]\n",
      " [ 0.49943584]\n",
      " [-0.30007377]\n",
      " [-0.12833865]]\n",
      "Training set error 28208.4\n"
     ]
    }
   ],
   "source": [
    "opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = opt.minimize(obj)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(train_op)\n",
    "    solution = theta.eval()\n",
    "    print(solution)\n",
    "    print(\"Training set error\", loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Implement Gradient Descent Linear SVM Classifier with Tensor Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following the example above, re-implement your Gradient Descent Linear SVM classifier using Tensor Flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First implement by providing its gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(np.c_[np.ones(nobs), boston_X], dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(boston_y, dtype=tf.float32, name=\"y\")\n",
    "\n",
    "lam = 1.0\n",
    "penalty_vec = np.vstack([[0.0], np.full((nfeatures, 1), lam)])\n",
    "\n",
    "learning_rate = 0.000001\n",
    "nepochs = 1000\n",
    "\n",
    "stddev = 2 / np.sqrt(nfeatures)\n",
    "\n",
    "theta = tf.Variable(tf.truncated_normal([nfeatures + 1, 1], stddev=stddev), name=\"theta\")\n",
    "\n",
    "f = tf.matmul(X, theta, name=\"f\")\n",
    "\n",
    "yf = y*f\n",
    "u=(1.0-yf)\n",
    "\n",
    "condition = tf.greater(u, 0.)\n",
    "pos_part = tf.where(condition, u, tf.zeros_like(u))\n",
    "\n",
    "loss=tf.reduce_mean(pos_part, name=\"loss\")\n",
    "obj = loss + 0.5 * tf.matmul(tf.transpose(theta), penalty_vec * theta)\n",
    "\n",
    "condition2 = tf.less(yf, 1.)\n",
    "t = tf.where(condition2, -tf.ones_like(yf), tf.zeros_like(yf))\n",
    "\n",
    "\n",
    "ty=t*y\n",
    "\n",
    "grad= (tf.matmul(tf.transpose(X), ty/nobs)) + penalty_vec * theta\n",
    "gradnorm = tf.norm(grad)\n",
    "\n",
    "\n",
    "update = tf.assign(theta, theta - learning_rate * grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 obj= [[ 4459.87646484]] grad_norm= 11980.0\n",
      "Epoch 100 obj= [[ 4.5340066]] grad_norm= 387.767\n",
      "Epoch 200 obj= [[ 1.0660162]] grad_norm= 1.46015\n",
      "Epoch 300 obj= [[ 1.06580532]] grad_norm= 1.46\n",
      "Epoch 400 obj= [[ 1.06559432]] grad_norm= 1.45986\n",
      "Epoch 500 obj= [[ 1.06538343]] grad_norm= 1.45971\n",
      "Epoch 600 obj= [[ 1.06517231]] grad_norm= 1.45957\n",
      "Epoch 700 obj= [[ 1.06496155]] grad_norm= 1.45943\n",
      "Epoch 800 obj= [[ 1.06475055]] grad_norm= 1.45928\n",
      "Epoch 900 obj= [[ 1.06453979]] grad_norm= 1.45914\n",
      "[[ 0.43312255]\n",
      " [ 0.19515757]\n",
      " [-0.48010713]\n",
      " [-0.17471494]\n",
      " [-0.36800322]\n",
      " [-0.05611996]\n",
      " [-0.97037131]\n",
      " [ 0.37912083]\n",
      " [ 0.59237027]\n",
      " [-0.09108316]\n",
      " [ 0.02185793]\n",
      " [ 0.48559609]\n",
      " [ 0.06975992]\n",
      " [-0.07258014]]\n",
      "Training set errror 0.0\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"obj=\", obj.eval(), \"grad_norm=\", gradnorm.eval())\n",
    "        sess.run(update)\n",
    "        \n",
    "    solution = theta.eval()\n",
    "    print(solution)\n",
    "    print(\"Training set errror\", loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, change it to use numerical differentiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 obj= [[ 4394.24365234]] grad_norm= 11980.0\n",
      "Epoch 100 obj= [[ 70.72247314]] grad_norm= 729.186\n",
      "Epoch 200 obj= [[ 30.6693058]] grad_norm= 544.161\n",
      "Epoch 300 obj= [[ 9.26455116]] grad_norm= 322.762\n",
      "Epoch 400 obj= [[ 3.13280988]] grad_norm= 165.625\n",
      "Epoch 500 obj= [[ 1.86560357]] grad_norm= 93.2578\n",
      "Epoch 600 obj= [[ 1.44370627]] grad_norm= 46.3753\n",
      "Epoch 700 obj= [[ 1.2312324]] grad_norm= 31.8941\n",
      "Epoch 800 obj= [[ 1.131387]] grad_norm= 20.11\n",
      "Epoch 900 obj= [[ 1.09094965]] grad_norm= 20.108\n",
      "[[-0.04730881]\n",
      " [-0.34313971]\n",
      " [ 0.08574782]\n",
      " [ 0.01171829]\n",
      " [ 0.82060707]\n",
      " [ 0.09238216]\n",
      " [-0.66020775]\n",
      " [ 0.34411767]\n",
      " [-0.0950948 ]\n",
      " [ 0.46359235]\n",
      " [-0.05423754]\n",
      " [ 0.25593981]\n",
      " [ 0.33474696]\n",
      " [ 0.57677305]]\n",
      "Training set error 0.00122579\n"
     ]
    }
   ],
   "source": [
    "auto_grad = tf.gradients(obj, [theta])[0]\n",
    "gradnorm = tf.norm(auto_grad)\n",
    "auto_update = tf.assign(theta, theta - learning_rate * auto_grad)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"obj=\", obj.eval(), \"grad_norm=\", gradnorm.eval())\n",
    "        sess.run(auto_update)\n",
    "    solution = theta.eval()\n",
    "    print(solution)\n",
    "    print(\"Training set error\", loss.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, use the included gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44889131]\n",
      " [ 0.92554706]\n",
      " [-0.09448642]\n",
      " [-0.14388104]\n",
      " [ 0.81002623]\n",
      " [-0.07992097]\n",
      " [-0.16729937]\n",
      " [ 0.47532859]\n",
      " [-0.71648061]\n",
      " [ 0.52153224]\n",
      " [-0.13271137]\n",
      " [ 0.00708812]\n",
      " [-0.67695987]\n",
      " [ 0.42074496]]\n",
      "Training set error 5964.36\n"
     ]
    }
   ],
   "source": [
    "opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = opt.minimize(obj)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(train_op)\n",
    "    solution = theta.eval()\n",
    "    print(solution)\n",
    "    print(\"Training set error\", loss.eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
